{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"conll2003\", trust_remote_code=True)\n",
    "\n",
    "train_dataset = dataset['train']\n",
    "valid_dataset = dataset['validation']\n",
    "test_dataset = dataset['test']\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-19T15:18:55.577220800Z",
     "start_time": "2025-03-19T15:18:54.625670200Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'document_id', 'sentence_id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "    num_rows: 14041\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-19T15:18:55.600172500Z",
     "start_time": "2025-03-19T15:18:55.578299800Z"
    }
   },
   "id": "dd56079084eb1061"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n"
     ]
    }
   ],
   "source": [
    "labels = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
    "num_labels = len(labels)\n",
    "print(labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-19T15:18:55.637842900Z",
     "start_time": "2025-03-19T15:18:55.594089700Z"
    }
   },
   "id": "51ea8d1dd1221da6"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-19T15:18:57.580546Z",
     "start_time": "2025-03-19T15:18:55.610356100Z"
    }
   },
   "id": "6e51e05d7d40e757"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=num_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-19T15:18:59.702576Z",
     "start_time": "2025-03-19T15:18:57.581545800Z"
    }
   },
   "id": "4dcaf1160f46ce94"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def batchify(data, bsz):\n",
    "    # Work out how cleanly we can divide the dataset into bsz parts\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit\n",
    "    data = data[:nbatch * bsz]\n",
    "    # Evenly divide the data across the bsz batches\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-19T15:18:59.747342500Z",
     "start_time": "2025-03-19T15:18:59.704690Z"
    }
   },
   "id": "880b52fc7e1317e6"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# batch_size = 20\n",
    "# eval_batch_size = 10\n",
    "# \n",
    "# \n",
    "# \n",
    "# # train_data = batchify(train_dataset, batch_size)\n",
    "# # val_data = batchify(valid_dataset, eval_batch_size)\n",
    "# # test_data = batchify(test_dataset, eval_batch_size)\n",
    "# \n",
    "# test = list(train_dataset.values())\n",
    "# print(test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-19T15:18:59.753443500Z",
     "start_time": "2025-03-19T15:18:59.718851200Z"
    }
   },
   "id": "6916aea53be7d9e1"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-19T15:19:00.640491300Z",
     "start_time": "2025-03-19T15:18:59.735175700Z"
    }
   },
   "id": "1ace5000cd89d09a"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.6023,  0.1092,  0.1417,  ..., -0.4177,  0.6059,  0.1764],\n",
      "         [ 0.5119, -0.4770,  0.5508,  ..., -0.2814,  0.3793,  0.1156],\n",
      "         [ 0.0995,  0.0867,  0.0869,  ...,  0.4789, -0.3236,  0.3122],\n",
      "         ...,\n",
      "         [ 0.8081, -0.7380,  0.2001,  ...,  0.7405, -0.7998,  0.6449],\n",
      "         [ 0.3305, -0.1958,  0.3148,  ..., -0.0525,  0.5358,  0.1987],\n",
      "         [ 0.5655, -0.2176, -0.4720,  ..., -0.3554,  0.6141, -0.2476]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-8.2835e-01,  6.0906e-01,  9.9998e-01, -9.9831e-01,  9.8739e-01,\n",
      "          9.2930e-01,  9.9664e-01, -9.8909e-01, -9.9124e-01, -7.8317e-01,\n",
      "          9.9476e-01,  9.9961e-01, -9.9850e-01, -9.9997e-01,  8.0649e-01,\n",
      "         -9.9098e-01,  9.9545e-01, -6.9968e-01, -9.9999e-01, -2.6556e-01,\n",
      "         -4.1867e-01, -9.9998e-01,  3.1562e-01,  9.7638e-01,  9.9257e-01,\n",
      "          1.4021e-01,  9.9543e-01,  9.9999e-01,  9.4214e-01, -2.0248e-01,\n",
      "          3.5963e-01, -9.9665e-01,  8.5152e-01, -9.9975e-01,  2.8570e-01,\n",
      "          7.8626e-03,  8.4606e-01, -3.7096e-01,  7.4315e-01, -9.5596e-01,\n",
      "         -7.8807e-01, -4.7754e-01,  6.0904e-01, -6.4658e-01,  9.6723e-01,\n",
      "          2.2952e-01,  2.4267e-01,  1.1464e-01, -1.1224e-01,  9.9997e-01,\n",
      "         -9.9227e-01,  9.9995e-01, -9.9455e-01,  9.9951e-01,  9.9903e-01,\n",
      "          4.6473e-01,  9.9825e-01,  1.8630e-01, -9.9897e-01,  4.2824e-01,\n",
      "          9.7877e-01,  9.9492e-02,  9.7601e-01, -2.5574e-01, -1.0145e-01,\n",
      "         -5.6067e-01, -8.6941e-01,  2.3935e-01, -6.2682e-01,  5.1608e-01,\n",
      "          4.8575e-01,  4.8747e-01,  9.9565e-01, -9.5639e-01, -1.1690e-01,\n",
      "         -9.4263e-01,  1.5285e-01, -9.9998e-01,  9.8651e-01,  9.9999e-01,\n",
      "          6.5605e-01, -9.9991e-01,  9.9870e-01, -4.0475e-01, -7.0561e-01,\n",
      "          2.7477e-01, -9.9917e-01, -9.9987e-01,  1.8895e-01, -6.6828e-01,\n",
      "          8.3371e-01, -9.9579e-01,  5.4993e-01, -9.1550e-01,  1.0000e+00,\n",
      "         -9.6308e-01, -2.8974e-01,  4.9713e-01,  9.5074e-01, -3.1329e-01,\n",
      "         -8.5073e-01,  9.2660e-01,  9.9924e-01, -9.9509e-01,  9.9877e-01,\n",
      "          5.7332e-01, -9.5204e-01, -8.4878e-01,  5.3101e-01,  2.2957e-01,\n",
      "          9.9633e-01, -9.9581e-01, -8.4645e-01,  2.3134e-01,  9.7124e-01,\n",
      "         -7.6645e-01,  9.9707e-01,  7.0794e-01, -4.3663e-01,  1.0000e+00,\n",
      "         -2.7179e-01,  9.8517e-01,  9.9961e-01,  9.0877e-01, -8.1168e-01,\n",
      "         -3.3681e-01, -4.8113e-01,  8.1254e-01, -3.3984e-01, -5.0071e-01,\n",
      "          8.5569e-01, -9.9618e-01, -9.9880e-01,  9.9988e-01, -3.4080e-01,\n",
      "          9.9999e-01, -9.9978e-01,  9.9567e-01, -9.9999e-01, -5.9676e-01,\n",
      "         -8.0271e-01, -1.6363e-02, -9.9063e-01,  2.9677e-01,  9.9670e-01,\n",
      "          1.9180e-01, -9.6363e-01, -6.3803e-01,  4.5937e-01, -8.5751e-01,\n",
      "          6.9634e-01,  8.6684e-01, -9.8712e-01,  9.9985e-01,  9.9661e-01,\n",
      "          9.6512e-01,  9.9064e-01,  3.0494e-01, -9.6835e-01,  8.8074e-01,\n",
      "          9.9611e-01, -9.9987e-01,  5.6770e-01, -9.9339e-01,  9.9983e-01,\n",
      "          9.9128e-01,  6.8117e-01, -9.9564e-01,  9.9998e-01, -4.5610e-01,\n",
      "          2.2760e-01, -1.9633e-01, -4.2445e-01, -9.9918e-01,  6.1411e-01,\n",
      "          5.4706e-01,  8.1350e-01,  9.9991e-01, -9.9806e-01,  9.9992e-01,\n",
      "          9.9405e-01, -2.4961e-01,  8.3604e-01,  9.9859e-01, -9.9852e-01,\n",
      "         -9.9445e-01, -9.9634e-01,  3.6493e-01,  5.9082e-01,  4.1785e-01,\n",
      "          4.1630e-01,  9.7932e-01,  9.9917e-01,  8.1510e-01, -9.9967e-01,\n",
      "         -4.6312e-01,  9.9122e-01, -2.8691e-01,  1.0000e+00,  2.9178e-01,\n",
      "         -9.9996e-01, -8.2507e-01,  9.6417e-01,  9.9644e-01, -4.4478e-01,\n",
      "          9.9324e-01, -5.7101e-01, -3.4857e-04,  9.8806e-01, -9.9977e-01,\n",
      "          9.9852e-01, -2.3978e-01,  8.4522e-01,  9.3744e-01,  9.9795e-01,\n",
      "         -8.0662e-01, -2.2260e-01,  3.8672e-01, -6.8129e-01,  9.9997e-01,\n",
      "         -9.9990e-01, -3.2183e-01,  5.9251e-01, -9.9829e-01, -9.9941e-01,\n",
      "          9.9586e-01, -1.0074e-01, -7.3508e-01, -2.5186e-01,  1.9904e-01,\n",
      "          3.7265e-01,  9.1428e-01,  9.9669e-01, -5.9080e-01, -2.5383e-01,\n",
      "         -9.9996e-01, -9.9760e-01, -8.7478e-01, -9.7038e-01,  2.0462e-01,\n",
      "          8.0368e-01, -5.2217e-01, -9.6130e-01, -9.9889e-01,  9.9059e-01,\n",
      "          6.4181e-01, -9.2359e-01, -5.0590e-01, -5.2063e-01, -9.9898e-01,\n",
      "          3.8437e-01, -8.1184e-01, -9.9977e-01,  9.9992e-01, -8.2068e-01,\n",
      "          9.9730e-01,  9.9512e-01, -9.9856e-01,  7.3626e-01, -9.9888e-01,\n",
      "         -7.3231e-03, -9.9984e-01,  1.2394e-01,  3.3718e-01, -7.3533e-01,\n",
      "         -8.3841e-02,  9.9802e-01, -9.8660e-01, -8.3092e-01,  8.1347e-01,\n",
      "         -9.9999e-01,  9.7834e-01, -3.6798e-01,  9.9975e-01,  7.8506e-01,\n",
      "         -7.4606e-02,  9.9465e-01,  9.3201e-01, -9.9514e-01, -9.9995e-01,\n",
      "          8.7159e-01,  9.9806e-01, -9.9809e-01, -3.5480e-01,  9.9999e-01,\n",
      "         -9.9877e-01, -8.3069e-01, -9.7494e-01, -9.9894e-01, -9.9993e-01,\n",
      "          1.5644e-01, -7.8414e-01,  6.0187e-02,  9.9392e-01,  2.4572e-01,\n",
      "          1.6016e-01,  9.9910e-01,  9.9941e-01,  2.2788e-01,  8.4298e-03,\n",
      "          1.5305e-01, -9.9167e-01, -9.9933e-01,  6.4604e-01,  3.7302e-01,\n",
      "         -9.9999e-01,  9.9997e-01, -9.9826e-01,  9.9987e-01,  9.6995e-01,\n",
      "         -9.9343e-01,  8.8091e-01,  1.4430e-02, -9.6750e-01,  1.0024e-01,\n",
      "          9.9998e-01,  9.9338e-01, -1.5025e-01,  3.1323e-01,  9.1669e-01,\n",
      "         -2.1946e-01,  4.9523e-01, -7.6086e-01, -5.7688e-01,  2.3562e-01,\n",
      "         -9.6419e-01,  9.9618e-01,  6.4859e-01, -9.9733e-01,  9.9700e-01,\n",
      "          6.5070e-02,  8.3284e-01, -7.4301e-01,  9.1747e-01,  9.9662e-01,\n",
      "         -2.7177e-01, -3.7245e-01, -4.3213e-02, -9.3129e-01, -9.7803e-01,\n",
      "          2.7836e-01, -9.9750e-01, -3.2621e-01,  9.6046e-01,  9.9694e-01,\n",
      "         -9.9700e-01,  9.9945e-01, -3.0036e-01,  8.7511e-01, -9.9853e-01,\n",
      "          1.0000e+00, -9.9977e-01,  2.8375e-01,  5.8693e-01, -9.1986e-01,\n",
      "         -5.8613e-01,  9.9782e-01,  9.8489e-01,  9.7992e-01, -8.6434e-01,\n",
      "         -4.6918e-01,  9.2471e-01,  9.9000e-01, -9.8676e-01,  5.5841e-02,\n",
      "         -9.9959e-01, -6.3961e-01,  9.9900e-01,  9.9601e-01, -1.8137e-01,\n",
      "         -5.7472e-01, -9.9791e-01,  9.8469e-01, -8.4956e-01, -7.2607e-01,\n",
      "         -1.8493e-01, -8.4716e-01,  5.6414e-01,  9.9854e-01, -2.8356e-01,\n",
      "          5.9711e-01,  2.0103e-01, -9.9755e-01,  8.3863e-01,  6.8787e-01,\n",
      "          9.9996e-01, -9.9299e-01,  3.1409e-01,  9.9636e-01, -3.3831e-01,\n",
      "         -6.6649e-01,  7.4109e-01,  9.9907e-01, -9.8940e-01, -3.6638e-01,\n",
      "         -9.9990e-01,  4.8964e-02, -8.5911e-01,  1.4593e-01, -2.9822e-01,\n",
      "          2.2590e-01, -8.4246e-01,  9.6926e-01,  1.0953e-01,  8.4557e-01,\n",
      "         -5.9910e-03,  9.8803e-01, -2.0996e-02, -1.0543e-01, -4.6601e-01,\n",
      "         -4.2787e-02,  6.1662e-01,  1.2547e-01,  9.9180e-01, -9.9351e-01,\n",
      "          9.9996e-01, -6.7802e-01, -9.9999e-01, -9.9770e-01, -5.7405e-01,\n",
      "         -9.9994e-01,  6.3853e-01, -9.9951e-01,  9.9676e-01,  9.6816e-01,\n",
      "         -9.9848e-01, -9.9946e-01, -9.9982e-01, -9.9976e-01,  6.1510e-01,\n",
      "          6.4182e-01, -2.1805e-01,  9.2797e-02,  9.0330e-01,  1.4380e-01,\n",
      "          1.2573e-01, -1.0409e-01, -9.7871e-01, -4.2303e-01, -9.9905e-01,\n",
      "          7.1276e-01, -9.9999e-01, -6.9909e-01,  9.9814e-01, -9.9562e-01,\n",
      "         -9.4415e-01, -9.6432e-01, -8.9964e-01, -9.2392e-01,  5.7695e-01,\n",
      "          9.9405e-01, -1.8583e-01, -5.1823e-01, -9.9992e-01,  9.9637e-01,\n",
      "         -8.6311e-01,  1.3339e-01, -8.4187e-01, -9.9070e-01,  9.9994e-01,\n",
      "          8.9447e-01, -1.2484e-01, -2.5460e-01, -9.9981e-01,  9.9253e-01,\n",
      "         -9.3657e-01, -9.1491e-01, -9.9574e-01,  3.4290e-01, -9.7082e-01,\n",
      "         -9.9998e-01,  1.6956e-01,  9.9788e-01,  9.9939e-01,  9.8977e-01,\n",
      "          1.9435e-01, -4.6757e-01, -9.7244e-01,  3.6547e-01, -9.9999e-01,\n",
      "          8.2877e-01,  8.5028e-01, -9.9369e-01, -5.5643e-01,  9.9792e-01,\n",
      "          9.9398e-01, -9.6296e-01, -9.8676e-01,  9.4785e-01,  7.4835e-01,\n",
      "          9.7754e-01, -3.5312e-01, -4.9721e-01,  4.5349e-01, -1.5718e-01,\n",
      "         -9.9632e-01, -9.8159e-01,  9.9876e-01, -9.9932e-01,  9.8976e-01,\n",
      "          9.9815e-01,  9.9910e-01,  2.7285e-01, -1.3299e-01, -9.9287e-01,\n",
      "         -9.9943e-01, -6.8430e-01,  3.2410e-01, -9.9999e-01,  9.9998e-01,\n",
      "         -1.0000e+00,  5.1310e-01, -6.4679e-01,  9.0606e-01,  9.9615e-01,\n",
      "         -3.3432e-01, -9.9998e-01, -9.9997e-01,  8.6537e-01, -3.1250e-01,\n",
      "          9.9661e-01,  2.3914e-01,  3.9441e-01, -5.5191e-01, -2.8731e-02,\n",
      "          9.9953e-01, -9.4059e-01, -6.8058e-01, -9.9860e-01,  9.9991e-01,\n",
      "          7.7975e-01, -9.9972e-01,  9.9650e-01, -9.9993e-01,  8.6067e-01,\n",
      "          9.9085e-01,  9.5399e-01,  9.9253e-01, -9.9939e-01,  1.0000e+00,\n",
      "         -9.9996e-01,  9.9958e-01, -9.9999e-01, -9.9914e-01,  9.9996e-01,\n",
      "         -9.9680e-01, -5.2242e-01, -9.9995e-01, -9.9846e-01,  7.6624e-01,\n",
      "          3.0813e-01, -6.0101e-01,  9.9681e-01, -9.9996e-01, -9.9965e-01,\n",
      "          5.0632e-01, -9.5096e-01, -7.7078e-01,  9.9660e-01, -5.1192e-01,\n",
      "          9.9864e-01, -1.3044e-02,  9.8215e-01,  1.4195e-01,  9.9789e-01,\n",
      "          9.9982e-01, -6.5892e-01, -4.9805e-01, -9.9783e-01,  9.9263e-01,\n",
      "         -6.6011e-01,  4.8956e-01,  9.8212e-01, -1.5472e-01, -4.5557e-01,\n",
      "          5.7543e-01, -9.9919e-01,  5.2152e-01, -8.3201e-01,  8.9039e-01,\n",
      "          9.4857e-01,  9.1789e-01,  1.1048e-02, -3.8681e-01, -1.1080e-01,\n",
      "         -9.9808e-01,  7.4150e-01, -9.9987e-01,  9.9106e-01, -9.6465e-01,\n",
      "          2.9540e-01, -5.3421e-01,  5.4674e-01, -9.7558e-01,  9.9991e-01,\n",
      "          9.9965e-01, -9.9994e-01,  2.2361e-01,  9.9665e-01, -6.4159e-01,\n",
      "          9.9177e-01, -9.9753e-01, -6.9763e-02,  9.6400e-01, -8.7434e-01,\n",
      "          9.9381e-01,  1.8743e-01, -2.3633e-01,  9.8364e-01, -9.9887e-01,\n",
      "         -9.0222e-01, -7.9110e-01,  3.5238e-01,  3.4325e-01, -9.8982e-01,\n",
      "          2.8097e-01,  9.8251e-01,  7.6614e-02, -9.9993e-01,  9.8067e-01,\n",
      "         -9.9986e-01, -3.6354e-01,  9.9292e-01,  1.4346e-01,  9.9998e-01,\n",
      "         -7.4159e-01,  3.1454e-02,  1.0397e-01, -9.9994e-01, -9.9913e-01,\n",
      "          2.1931e-01, -2.7733e-01, -9.5555e-01,  9.9938e-01, -5.0607e-02,\n",
      "          8.3410e-01, -9.9998e-01,  4.2417e-01,  9.9698e-01,  3.9198e-01,\n",
      "          9.1816e-01, -8.0208e-01, -9.7382e-01, -9.5518e-01, -6.6141e-01,\n",
      "          1.5180e-01,  9.0137e-01, -9.9532e-01, -9.1342e-01, -7.1074e-01,\n",
      "          9.9999e-01, -9.9956e-01, -9.7606e-01, -9.9647e-01,  4.1117e-01,\n",
      "          8.8649e-01,  6.1946e-01,  1.1032e-01, -8.1373e-01,  9.3144e-01,\n",
      "         -9.1679e-01,  9.9910e-01, -9.9854e-01, -9.9933e-01,  9.9996e-01,\n",
      "          6.8804e-01, -9.9306e-01, -1.1649e-01, -4.1921e-01,  7.9799e-02,\n",
      "         -6.4763e-03,  7.7816e-01, -9.5855e-01, -2.6136e-01, -9.9975e-01,\n",
      "          8.5560e-01, -8.4889e-01, -9.9709e-01, -6.7251e-01, -4.8243e-01,\n",
      "         -9.9988e-01,  9.9809e-01,  9.9115e-01,  9.9999e-01, -9.9996e-01,\n",
      "          8.7347e-01,  2.2774e-01,  9.9976e-01,  4.1655e-02, -7.7174e-01,\n",
      "          9.2765e-01,  9.9992e-01, -7.4625e-01,  7.7272e-01,  1.8972e-02,\n",
      "         -2.1511e-01,  4.3176e-02, -7.1345e-01,  9.9652e-01, -9.4827e-01,\n",
      "          3.5106e-01, -9.9568e-01, -9.9998e-01,  9.9999e-01, -1.2416e-01,\n",
      "          9.9711e-01,  3.3779e-01,  8.0375e-01, -8.9710e-01,  9.8400e-01,\n",
      "         -9.8929e-01, -9.3701e-01, -1.0000e+00,  2.1380e-01, -9.9995e-01,\n",
      "         -9.9693e-01,  2.9115e-01,  9.9771e-01, -9.9989e-01, -9.9722e-01,\n",
      "         -3.7007e-01, -1.0000e+00,  9.1409e-01, -9.9338e-01, -7.9647e-01,\n",
      "         -9.9685e-01,  9.9875e-01, -5.3951e-01, -4.7717e-01,  9.9424e-01,\n",
      "         -9.9099e-01,  9.3857e-01,  9.7499e-01,  7.2225e-01,  2.9207e-01,\n",
      "          3.1667e-01, -6.4539e-01, -9.9575e-01, -9.5048e-01, -9.8199e-01,\n",
      "          9.2237e-01, -9.9711e-01, -9.4075e-01,  9.9913e-01,  9.9735e-01,\n",
      "         -9.9988e-01, -9.9907e-01,  9.9719e-01, -2.8665e-01,  9.9731e-01,\n",
      "         -6.8889e-01, -9.9997e-01, -9.9998e-01,  1.8779e-01, -2.7697e-01,\n",
      "          9.9849e-01, -4.6101e-01,  9.9961e-01,  8.1148e-01, -8.5870e-02,\n",
      "          4.4598e-01, -3.9436e-01, -1.2326e-01, -2.3773e-01, -2.7982e-01,\n",
      "          9.9999e-01, -4.9552e-01,  9.9713e-01]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-19T15:19:00.657898400Z",
     "start_time": "2025-03-19T15:19:00.642566100Z"
    }
   },
   "id": "e3bd30ac1b4f36d1"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'LABEL_1', 'score': np.float32(0.5220458), 'index': 1, 'word': 'my', 'start': 0, 'end': 2}, {'entity': 'LABEL_0', 'score': np.float32(0.7009999), 'index': 2, 'word': 'name', 'start': 3, 'end': 7}, {'entity': 'LABEL_0', 'score': np.float32(0.6081854), 'index': 3, 'word': 'is', 'start': 8, 'end': 10}, {'entity': 'LABEL_0', 'score': np.float32(0.5473961), 'index': 4, 'word': 'wolfgang', 'start': 11, 'end': 19}, {'entity': 'LABEL_0', 'score': np.float32(0.6817879), 'index': 5, 'word': 'and', 'start': 20, 'end': 23}, {'entity': 'LABEL_0', 'score': np.float32(0.55994314), 'index': 6, 'word': 'i', 'start': 24, 'end': 25}, {'entity': 'LABEL_0', 'score': np.float32(0.61987853), 'index': 7, 'word': 'live', 'start': 26, 'end': 30}, {'entity': 'LABEL_0', 'score': np.float32(0.7031592), 'index': 8, 'word': 'in', 'start': 31, 'end': 33}, {'entity': 'LABEL_1', 'score': np.float32(0.5161929), 'index': 9, 'word': 'berlin', 'start': 34, 'end': 40}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "example = \"My name is Wolfgang and I live in Berlin\"\n",
    "\n",
    "ner_results = nlp(example)\n",
    "print(ner_results)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-19T15:19:02.023438600Z",
     "start_time": "2025-03-19T15:19:00.657898400Z"
    }
   },
   "id": "aab526785c81681a"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SOCCER', '-', 'JAPAN', 'GET', 'LUCKY', 'WIN', ',', 'CHINA', 'IN', 'SURPRISE', 'DEFEAT', '.']\n",
      "['SOCCER', '-', 'JAPAN', 'GET', 'LUCKY', 'WIN', ',', 'CHINA', 'IN', 'SURPRISE', 'DEFEAT', '.']\n",
      "['SOCCER', '-', 'JAPAN', 'GET', 'LUCKY', 'WIN', ',', 'CHINA', 'IN', 'SURPRISE', 'DEFEAT', '.']\n",
      "['Japan', 'began', 'the', 'defence', 'of', 'their', 'Asian', 'Cup', 'title', 'with', 'a', 'lucky', '2-1', 'win', 'against', 'Syria', 'in', 'a', 'Group', 'C', 'championship', 'match', 'on', 'Friday', '.']\n",
      "['Japan', 'began', 'the', 'defence', 'of', 'their', 'Asian', 'Cup', 'title', 'with', 'a', 'lucky', '2-1', 'win', 'against', 'Syria', 'in', 'a', 'Group', 'C', 'championship', 'match', 'on', 'Friday', '.']\n",
      "['Japan', 'began', 'the', 'defence', 'of', 'their', 'Asian', 'Cup', 'title', 'with', 'a', 'lucky', '2-1', 'win', 'against', 'Syria', 'in', 'a', 'Group', 'C', 'championship', 'match', 'on', 'Friday', '.']\n",
      "['Japan', 'began', 'the', 'defence', 'of', 'their', 'Asian', 'Cup', 'title', 'with', 'a', 'lucky', '2-1', 'win', 'against', 'Syria', 'in', 'a', 'Group', 'C', 'championship', 'match', 'on', 'Friday', '.']\n",
      "['But', 'China', 'saw', 'their', 'luck', 'desert', 'them', 'in', 'the', 'second', 'match', 'of', 'the', 'group', ',', 'crashing', 'to', 'a', 'surprise', '2-0', 'defeat', 'to', 'newcomers', 'Uzbekistan', '.']\n",
      "['But', 'China', 'saw', 'their', 'luck', 'desert', 'them', 'in', 'the', 'second', 'match', 'of', 'the', 'group', ',', 'crashing', 'to', 'a', 'surprise', '2-0', 'defeat', 'to', 'newcomers', 'Uzbekistan', '.']\n",
      "['But', 'China', 'saw', 'their', 'luck', 'desert', 'them', 'in', 'the', 'second', 'match', 'of', 'the', 'group', ',', 'crashing', 'to', 'a', 'surprise', '2-0', 'defeat', 'to', 'newcomers', 'Uzbekistan', '.']\n",
      "['But', 'China', 'saw', 'their', 'luck', 'desert', 'them', 'in', 'the', 'second', 'match', 'of', 'the', 'group', ',', 'crashing', 'to', 'a', 'surprise', '2-0', 'defeat', 'to', 'newcomers', 'Uzbekistan', '.']\n",
      "['China', 'controlled', 'most', 'of', 'the', 'match', 'and', 'saw', 'several', 'chances', 'missed', 'until', 'the', '78th', 'minute', 'when', 'Uzbek', 'striker', 'Igor', 'Shkvyrin', 'took', 'advantage', 'of', 'a', 'misdirected', 'defensive', 'header', 'to', 'lob', 'the', 'ball', 'over', 'the', 'advancing', 'Chinese', 'keeper', 'and', 'into', 'an', 'empty', 'net', '.']\n",
      "['China', 'controlled', 'most', 'of', 'the', 'match', 'and', 'saw', 'several', 'chances', 'missed', 'until', 'the', '78th', 'minute', 'when', 'Uzbek', 'striker', 'Igor', 'Shkvyrin', 'took', 'advantage', 'of', 'a', 'misdirected', 'defensive', 'header', 'to', 'lob', 'the', 'ball', 'over', 'the', 'advancing', 'Chinese', 'keeper', 'and', 'into', 'an', 'empty', 'net', '.']\n",
      "['China', 'controlled', 'most', 'of', 'the', 'match', 'and', 'saw', 'several', 'chances', 'missed', 'until', 'the', '78th', 'minute', 'when', 'Uzbek', 'striker', 'Igor', 'Shkvyrin', 'took', 'advantage', 'of', 'a', 'misdirected', 'defensive', 'header', 'to', 'lob', 'the', 'ball', 'over', 'the', 'advancing', 'Chinese', 'keeper', 'and', 'into', 'an', 'empty', 'net', '.']\n",
      "['China', 'controlled', 'most', 'of', 'the', 'match', 'and', 'saw', 'several', 'chances', 'missed', 'until', 'the', '78th', 'minute', 'when', 'Uzbek', 'striker', 'Igor', 'Shkvyrin', 'took', 'advantage', 'of', 'a', 'misdirected', 'defensive', 'header', 'to', 'lob', 'the', 'ball', 'over', 'the', 'advancing', 'Chinese', 'keeper', 'and', 'into', 'an', 'empty', 'net', '.']\n",
      "['China', 'controlled', 'most', 'of', 'the', 'match', 'and', 'saw', 'several', 'chances', 'missed', 'until', 'the', '78th', 'minute', 'when', 'Uzbek', 'striker', 'Igor', 'Shkvyrin', 'took', 'advantage', 'of', 'a', 'misdirected', 'defensive', 'header', 'to', 'lob', 'the', 'ball', 'over', 'the', 'advancing', 'Chinese', 'keeper', 'and', 'into', 'an', 'empty', 'net', '.']\n",
      "['China', 'controlled', 'most', 'of', 'the', 'match', 'and', 'saw', 'several', 'chances', 'missed', 'until', 'the', '78th', 'minute', 'when', 'Uzbek', 'striker', 'Igor', 'Shkvyrin', 'took', 'advantage', 'of', 'a', 'misdirected', 'defensive', 'header', 'to', 'lob', 'the', 'ball', 'over', 'the', 'advancing', 'Chinese', 'keeper', 'and', 'into', 'an', 'empty', 'net', '.']\n",
      "['China', 'controlled', 'most', 'of', 'the', 'match', 'and', 'saw', 'several', 'chances', 'missed', 'until', 'the', '78th', 'minute', 'when', 'Uzbek', 'striker', 'Igor', 'Shkvyrin', 'took', 'advantage', 'of', 'a', 'misdirected', 'defensive', 'header', 'to', 'lob', 'the', 'ball', 'over', 'the', 'advancing', 'Chinese', 'keeper', 'and', 'into', 'an', 'empty', 'net', '.']\n",
      "['China', 'controlled', 'most', 'of', 'the', 'match', 'and', 'saw', 'several', 'chances', 'missed', 'until', 'the', '78th', 'minute', 'when', 'Uzbek', 'striker', 'Igor', 'Shkvyrin', 'took', 'advantage', 'of', 'a', 'misdirected', 'defensive', 'header', 'to', 'lob', 'the', 'ball', 'over', 'the', 'advancing', 'Chinese', 'keeper', 'and', 'into', 'an', 'empty', 'net', '.']\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 11\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m data \u001B[38;5;129;01min\u001B[39;00m test_dataset:\n\u001B[0;32m     10\u001B[0m     seq \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin([token \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtokens\u001B[39m\u001B[38;5;124m'\u001B[39m]])\n\u001B[1;32m---> 11\u001B[0m     ner_results \u001B[38;5;241m=\u001B[39m \u001B[43mnlp\u001B[49m\u001B[43m(\u001B[49m\u001B[43mseq\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     13\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(ner_results)):\n\u001B[0;32m     14\u001B[0m         \u001B[38;5;28mprint\u001B[39m(data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtokens\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "File \u001B[1;32m~\\PycharmProjects\\aist3120\\proj\\aist3120_project\\venv\\lib\\site-packages\\transformers\\pipelines\\token_classification.py:250\u001B[0m, in \u001B[0;36mTokenClassificationPipeline.__call__\u001B[1;34m(self, inputs, **kwargs)\u001B[0m\n\u001B[0;32m    247\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m offset_mapping:\n\u001B[0;32m    248\u001B[0m     kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moffset_mapping\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m offset_mapping\n\u001B[1;32m--> 250\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m(inputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\PycharmProjects\\aist3120\\proj\\aist3120_project\\venv\\lib\\site-packages\\transformers\\pipelines\\base.py:1360\u001B[0m, in \u001B[0;36mPipeline.__call__\u001B[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1358\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39miterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001B[0;32m   1359\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mframework \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m, ChunkPipeline):\n\u001B[1;32m-> 1360\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1361\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43miter\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1362\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_iterator\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1363\u001B[0m \u001B[43m                \u001B[49m\u001B[43m[\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_workers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpreprocess_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mforward_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpostprocess_params\u001B[49m\n\u001B[0;32m   1364\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1365\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1366\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1367\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1368\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001B[1;32m~\\PycharmProjects\\aist3120\\proj\\aist3120_project\\venv\\lib\\site-packages\\transformers\\pipelines\\pt_utils.py:124\u001B[0m, in \u001B[0;36mPipelineIterator.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    121\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloader_batch_item()\n\u001B[0;32m    123\u001B[0m \u001B[38;5;66;03m# We're out of items within a batch\u001B[39;00m\n\u001B[1;32m--> 124\u001B[0m item \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    125\u001B[0m processed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfer(item, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparams)\n\u001B[0;32m    126\u001B[0m \u001B[38;5;66;03m# We now have a batch of \"inferred things\".\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\aist3120\\proj\\aist3120_project\\venv\\lib\\site-packages\\transformers\\pipelines\\pt_utils.py:269\u001B[0m, in \u001B[0;36mPipelinePackIterator.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    266\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m accumulator\n\u001B[0;32m    268\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_last:\n\u001B[1;32m--> 269\u001B[0m     processed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfer(\u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39miterator), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparams)\n\u001B[0;32m    270\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloader_batch_size \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    271\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(processed, torch\u001B[38;5;241m.\u001B[39mTensor):\n",
      "File \u001B[1;32m~\\PycharmProjects\\aist3120\\proj\\aist3120_project\\venv\\lib\\site-packages\\transformers\\pipelines\\base.py:1275\u001B[0m, in \u001B[0;36mPipeline.forward\u001B[1;34m(self, model_inputs, **forward_params)\u001B[0m\n\u001B[0;32m   1273\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m inference_context():\n\u001B[0;32m   1274\u001B[0m         model_inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ensure_tensor_on_device(model_inputs, device\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m-> 1275\u001B[0m         model_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward(model_inputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mforward_params)\n\u001B[0;32m   1276\u001B[0m         model_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ensure_tensor_on_device(model_outputs, device\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[0;32m   1277\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\PycharmProjects\\aist3120\\proj\\aist3120_project\\venv\\lib\\site-packages\\transformers\\pipelines\\token_classification.py:287\u001B[0m, in \u001B[0;36mTokenClassificationPipeline._forward\u001B[1;34m(self, model_inputs)\u001B[0m\n\u001B[0;32m    285\u001B[0m     logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_inputs)[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    286\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 287\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_inputs)\n\u001B[0;32m    288\u001B[0m     logits \u001B[38;5;241m=\u001B[39m output[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogits\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(output, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m output[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    290\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m {\n\u001B[0;32m    291\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogits\u001B[39m\u001B[38;5;124m\"\u001B[39m: logits,\n\u001B[0;32m    292\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspecial_tokens_mask\u001B[39m\u001B[38;5;124m\"\u001B[39m: special_tokens_mask,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    296\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_inputs,\n\u001B[0;32m    297\u001B[0m }\n",
      "File \u001B[1;32m~\\PycharmProjects\\aist3120\\proj\\aist3120_project\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\PycharmProjects\\aist3120\\proj\\aist3120_project\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\PycharmProjects\\aist3120\\proj\\aist3120_project\\venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1867\u001B[0m, in \u001B[0;36mBertForTokenClassification.forward\u001B[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m   1861\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1862\u001B[0m \u001B[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001B[39;00m\n\u001B[0;32m   1863\u001B[0m \u001B[38;5;124;03m    Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\u001B[39;00m\n\u001B[0;32m   1864\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1865\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[1;32m-> 1867\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbert\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1868\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1869\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1870\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1871\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1872\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1873\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1874\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1875\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1876\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1877\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1879\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m   1881\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(sequence_output)\n",
      "File \u001B[1;32m~\\PycharmProjects\\aist3120\\proj\\aist3120_project\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\PycharmProjects\\aist3120\\proj\\aist3120_project\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\PycharmProjects\\aist3120\\proj\\aist3120_project\\venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1108\u001B[0m, in \u001B[0;36mBertModel.forward\u001B[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m   1101\u001B[0m         extended_attention_mask \u001B[38;5;241m=\u001B[39m _prepare_4d_causal_attention_mask_for_sdpa(\n\u001B[0;32m   1102\u001B[0m             attention_mask,\n\u001B[0;32m   1103\u001B[0m             input_shape,\n\u001B[0;32m   1104\u001B[0m             embedding_output,\n\u001B[0;32m   1105\u001B[0m             past_key_values_length,\n\u001B[0;32m   1106\u001B[0m         )\n\u001B[0;32m   1107\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1108\u001B[0m         extended_attention_mask \u001B[38;5;241m=\u001B[39m \u001B[43m_prepare_4d_attention_mask_for_sdpa\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1109\u001B[0m \u001B[43m            \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43membedding_output\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtgt_len\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mseq_length\u001B[49m\n\u001B[0;32m   1110\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1111\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1112\u001B[0m     \u001B[38;5;66;03m# We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\u001B[39;00m\n\u001B[0;32m   1113\u001B[0m     \u001B[38;5;66;03m# ourselves in which case we just need to make it broadcastable to all heads.\u001B[39;00m\n\u001B[0;32m   1114\u001B[0m     extended_attention_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_extended_attention_mask(attention_mask, input_shape)\n",
      "File \u001B[1;32m~\\PycharmProjects\\aist3120\\proj\\aist3120_project\\venv\\lib\\site-packages\\transformers\\modeling_attn_mask_utils.py:448\u001B[0m, in \u001B[0;36m_prepare_4d_attention_mask_for_sdpa\u001B[1;34m(mask, dtype, tgt_len)\u001B[0m\n\u001B[0;32m    445\u001B[0m is_tracing \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mjit\u001B[38;5;241m.\u001B[39mis_tracing() \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mask, torch\u001B[38;5;241m.\u001B[39mfx\u001B[38;5;241m.\u001B[39mProxy) \u001B[38;5;129;01mor\u001B[39;00m is_torchdynamo_compiling()\n\u001B[0;32m    447\u001B[0m \u001B[38;5;66;03m# torch.jit.trace, symbolic_trace and torchdynamo with fullgraph=True are unable to capture data-dependent controlflows.\u001B[39;00m\n\u001B[1;32m--> 448\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_tracing \u001B[38;5;129;01mand\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mall(mask \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m    449\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    450\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "def custom_tokenize(text):\n",
    "    return [token + ' ' for token in text.split()]\n",
    "\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "count = 0\n",
    "total = len(test_dataset)\n",
    "for data in test_dataset:\n",
    "    seq = \" \".join([token for token in data['tokens']])\n",
    "    ner_results = nlp(seq)\n",
    "\n",
    "    for i in range(len(ner_results)):\n",
    "        # print(ner_results[i]['entity'])\n",
    "        # print(data)\n",
    "        \n",
    "        if ner_results[i]['entity'] == data['ner_tags'][i]:\n",
    "            count += 1\n",
    "\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-19T15:19:03.709453300Z",
     "start_time": "2025-03-19T15:19:02.025440600Z"
    }
   },
   "id": "5aac1ac37197fa9f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(count/total)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-03-19T15:19:03.705347600Z"
    }
   },
   "id": "2d191b1c3c785ec9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Input text\n",
    "text = \"SOCCER - JAPAN GET LUCKY WIN , CHINA IN SURPRISE DEFEAT .\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "# Printing the tokens\n",
    "print(tokens)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae5f743c343bdd24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f4d46480dfba19"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
