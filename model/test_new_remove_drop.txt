DatasetDict({
    train: Dataset({
        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],
        num_rows: 14041
    })
    validation: Dataset({
        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],
        num_rows: 3250
    })
    test: Dataset({
        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],
        num_rows: 3453
    })
})
{'input_ids': [101, 7270, 22961, 1528, 1840, 1106, 21423, 1418, 2495, 12913, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}
Class Weights: tensor([0.0020, 0.0511, 0.0744, 0.0533, 0.0910, 0.0472, 0.2913, 0.0980, 0.2918])
Training in epoch_size: 30, batch_size: 12, learning_rate: 5e-06, device: cuda
vocab_size=28996, embed_size=768, num_layers=6, heads=8, forward_expansion=8, dropout=0.2, max_len=512, num_classes=9
Epoch 1, Loss: 2.1264
Epoch 2, Loss: 1.8280
Epoch 3, Loss: 1.6680
Epoch 4, Loss: 1.5489
Epoch 5, Loss: 1.4579
Epoch 6, Loss: 1.3859
Epoch 7, Loss: 1.3354
Epoch 8, Loss: 1.2875
Epoch 9, Loss: 1.2223
Epoch 10, Loss: 1.1871
Epoch 11, Loss: 1.1491
Epoch 12, Loss: 1.1077
Epoch 13, Loss: 1.0837
Epoch 14, Loss: 1.0522
Epoch 15, Loss: 1.0202
Epoch 16, Loss: 0.9908
Epoch 17, Loss: 0.9548
Epoch 18, Loss: 0.9332
Epoch 19, Loss: 0.9081
Epoch 20, Loss: 0.8847
Epoch 21, Loss: 0.8516
Epoch 22, Loss: 0.8363
Epoch 23, Loss: 0.8168
Epoch 24, Loss: 0.7915
Epoch 25, Loss: 0.7674
Epoch 26, Loss: 0.7534
Epoch 27, Loss: 0.7339
Epoch 28, Loss: 0.7180
Epoch 29, Loss: 0.6936
Epoch 30, Loss: 0.6815
Evaluation Report on Training Data:
              precision    recall  f1-score   support

           0     0.9974    0.5608    0.7180    205042
           1     0.3593    0.4828    0.4120     12148
           2     0.3099    0.8928    0.4601     10135
           3     0.4322    0.7233    0.5411     13253
           4     0.2126    0.8713    0.3418      6020
           5     0.6860    0.7710    0.7260     13243
           6     0.2371    0.9840    0.3821      1691
           7     0.2179    0.8107    0.3435      5885
           8     0.1086    0.9954    0.1959      1955

    accuracy                         0.6063    269372
   macro avg     0.3957    0.7880    0.4578    269372
weighted avg     0.8539    0.6063    0.6637    269372

Evaluation Report on Validation Data:
              precision    recall  f1-score   support

           0     0.9911    0.5434    0.7019     50662
           1     0.2985    0.4023    0.3427      3169
           2     0.2711    0.7614    0.3998      2779
           3     0.3196    0.6062    0.4186      2679
           4     0.1214    0.6091    0.2024      1059
           5     0.6198    0.6494    0.6343      3588
           6     0.1920    0.7143    0.3027       371
           7     0.1519    0.6480    0.2461      1463
           8     0.0850    0.6884    0.1514       584

    accuracy                         0.5596     66354
   macro avg     0.3390    0.6247    0.3778     66354
weighted avg     0.8359    0.5596    0.6319     66354

Evaluation Report on Test Data:
              precision    recall  f1-score   support

           0     0.9916    0.5585    0.7145     47232
           1     0.2740    0.3277    0.2984      2875
           2     0.2723    0.7419    0.3984      2511
           3     0.3513    0.6352    0.4524      3487
           4     0.1281    0.5802    0.2099      1284
           5     0.5990    0.6416    0.6196      2938
           6     0.1473    0.5109    0.2287       413
           7     0.1384    0.5363    0.2200      1255
           8     0.0547    0.6625    0.1011       320

    accuracy                         0.5636     62315
   macro avg     0.3285    0.5772    0.3603     62315
weighted avg     0.8298    0.5636    0.6367     62315

