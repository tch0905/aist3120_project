DatasetDict({
    train: Dataset({
        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],
        num_rows: 14041
    })
    validation: Dataset({
        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],
        num_rows: 3250
    })
    test: Dataset({
        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],
        num_rows: 3453
    })
})
{'input_ids': [101, 7270, 22961, 1528, 1840, 1106, 21423, 1418, 2495, 12913, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}
Class Weights: tensor([0.0020, 0.0511, 0.0744, 0.0533, 0.0910, 0.0472, 0.2913, 0.0980, 0.2918])
Training in epoch_size: 5, batch_size: 4, learning_rate: 3e-05, device: cuda
vocab_size=28996, embed_size=768, num_layers=6, heads=8, forward_expansion=8, dropout=0.8, max_len=512, num_classes=9
Epoch 1, Loss: 0.5183
Epoch 2, Loss: 0.2770
Epoch 3, Loss: 0.1752
Epoch 4, Loss: 0.1148
Epoch 5, Loss: 0.0720
Evaluation Report on Training Data:
              precision    recall  f1-score   support

           0     0.9955    0.9957    0.9956    205042
           1     0.8477    0.9816    0.9097     12148
           2     0.9724    0.9233    0.9472     10135
           3     0.9754    0.9305    0.9524     13253
           4     0.9571    0.9296    0.9431      6020
           5     0.9832    0.9472    0.9649     13243
           6     0.9741    0.8681    0.9181      1691
           7     0.9287    0.9341    0.9314      5885
           8     0.9423    0.9182    0.9301      1955

    accuracy                         0.9826    269372
   macro avg     0.9529    0.9365    0.9436    269372
weighted avg     0.9835    0.9826    0.9827    269372

Evaluation Report on Validation Data:
              precision    recall  f1-score   support

           0     0.9641    0.9854    0.9746     50662
           1     0.5598    0.8040    0.6600      3169
           2     0.8286    0.6243    0.7121      2779
           3     0.8158    0.6663    0.7335      2679
           4     0.7914    0.5411    0.6427      1059
           5     0.9160    0.7595    0.8304      3588
           6     0.8484    0.6334    0.7253       371
           7     0.7198    0.6671    0.6924      1463
           8     0.7990    0.5582    0.6573       584

    accuracy                         0.9167     66354
   macro avg     0.8048    0.6933    0.7365     66354
weighted avg     0.9203    0.9167    0.9154     66354

Evaluation Report on Test Data:
              precision    recall  f1-score   support

           0     0.9572    0.9702    0.9636     47232
           1     0.5022    0.7656    0.6065      2875
           2     0.7819    0.5611    0.6534      2511
           3     0.7455    0.6375    0.6873      3487
           4     0.6223    0.5467    0.5821      1284
           5     0.8386    0.7342    0.7829      2938
           6     0.7767    0.4044    0.5318       413
           7     0.6328    0.5108    0.5653      1255
           8     0.4551    0.4906    0.4722       320

    accuracy                         0.8903     62315
   macro avg     0.7014    0.6246    0.6495     62315
weighted avg     0.8945    0.8903    0.8894     62315

