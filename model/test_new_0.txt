DatasetDict({
    train: Dataset({
        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],
        num_rows: 14041
    })
    validation: Dataset({
        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],
        num_rows: 3250
    })
    test: Dataset({
        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],
        num_rows: 3453
    })
})
{'input_ids': [101, 7270, 22961, 1528, 1840, 1106, 21423, 1418, 2495, 12913, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}
Class Weights: tensor([0.0020, 0.0511, 0.0744, 0.0533, 0.0910, 0.0472, 0.2913, 0.0980, 0.2918])
Training in epoch_size: 30, batch_size: 12, learning_rate: 5e-06, device: cuda
vocab_size=28996, embed_size=768, num_layers=6, heads=8, forward_expansion=8, dropout=0.2, max_len=512, num_classes=9
Epoch 1, Loss: 2.1916
Epoch 2, Loss: 1.8971
Epoch 3, Loss: 1.7236
Epoch 4, Loss: 1.6117
Epoch 5, Loss: 1.5163
Epoch 6, Loss: 1.4359
Epoch 7, Loss: 1.3715
Epoch 8, Loss: 1.3224
Epoch 9, Loss: 1.2695
Epoch 10, Loss: 1.2223
Epoch 11, Loss: 1.1881
Epoch 12, Loss: 1.1490
Epoch 13, Loss: 1.1090
Epoch 14, Loss: 1.0794
Epoch 15, Loss: 1.0532
Epoch 16, Loss: 1.0183
Epoch 17, Loss: 0.9785
Epoch 18, Loss: 0.9550
Epoch 19, Loss: 0.9304
Epoch 20, Loss: 0.9046
Epoch 21, Loss: 0.8754
Epoch 22, Loss: 0.8594
Epoch 23, Loss: 0.8362
Epoch 24, Loss: 0.8173
Epoch 25, Loss: 0.7889
Epoch 26, Loss: 0.7676
Epoch 27, Loss: 0.7548
Epoch 28, Loss: 0.7336
Epoch 29, Loss: 0.7129
Epoch 30, Loss: 0.6921
Evaluation Report on Training Data:
              precision    recall  f1-score   support

           0     0.9969    0.6021    0.7508    205042
           1     0.3023    0.5386    0.3873     12148
           2     0.2715    0.8962    0.4168     10135
           3     0.5031    0.6885    0.5814     13253
           4     0.2715    0.8360    0.4099      6020
           5     0.4935    0.8396    0.6216     13243
           6     0.2841    0.9894    0.4414      1691
           7     0.2894    0.8314    0.4294      5885
           8     0.2278    0.9826    0.3699      1955

    accuracy                         0.6417    269372
   macro avg     0.4045    0.8005    0.4898    269372
weighted avg     0.8475    0.6417    0.6878    269372

Evaluation Report on Validation Data:
              precision    recall  f1-score   support

           0     0.9911    0.5873    0.7375     50662
           1     0.2417    0.4484    0.3141      3169
           2     0.2321    0.7701    0.3566      2779
           3     0.3798    0.5405    0.4461      2679
           4     0.1585    0.5552    0.2466      1059
           5     0.4432    0.7271    0.5507      3588
           6     0.2124    0.7278    0.3289       371
           7     0.2110    0.6582    0.3195      1463
           8     0.1916    0.6524    0.2962       584

    accuracy                         0.5964     66354
   macro avg     0.3401    0.6297    0.3996     66354
weighted avg     0.8274    0.5964    0.6563     66354

Evaluation Report on Test Data:
              precision    recall  f1-score   support

           0     0.9912    0.5894    0.7392     47232
           1     0.2253    0.3993    0.2881      2875
           2     0.2425    0.7710    0.3689      2511
           3     0.4048    0.5985    0.4829      3487
           4     0.1754    0.5592    0.2670      1284
           5     0.4114    0.7389    0.5285      2938
           6     0.1822    0.5811    0.2775       413
           7     0.1771    0.5100    0.2629      1255
           8     0.1117    0.5906    0.1879       320

    accuracy                         0.5932     62315
   macro avg     0.3246    0.5931    0.3781     62315
weighted avg     0.8225    0.5932    0.6540     62315

