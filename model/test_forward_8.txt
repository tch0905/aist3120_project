DatasetDict({
    train: Dataset({
        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],
        num_rows: 14041
    })
    validation: Dataset({
        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],
        num_rows: 3250
    })
    test: Dataset({
        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],
        num_rows: 3453
    })
})
{'input_ids': [101, 7270, 22961, 1528, 1840, 1106, 21423, 1418, 2495, 12913, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}
Class Weights: tensor([0.0020, 0.0511, 0.0744, 0.0533, 0.0910, 0.0472, 0.2913, 0.0980, 0.2918])
Training in epoch_size: 5, batch_size: 4, learning_rate: 3e-05, device: cuda
vocab_size=28996, embed_size=768, num_layers=6, heads=8, forward_expansion=8, dropout=0.1, max_len=512, num_classes=9
Epoch 1, Loss: 0.5230
Epoch 2, Loss: 0.2845
Epoch 3, Loss: 0.1773
Epoch 4, Loss: 0.1102
Epoch 5, Loss: 0.0698
Evaluation Report on Training Data:
              precision    recall  f1-score   support

           0     0.9957    0.9954    0.9955    205042
           1     0.9527    0.9622    0.9574     12148
           2     0.9473    0.9714    0.9592     10135
           3     0.9705    0.9417    0.9558     13253
           4     0.9674    0.9130    0.9394      6020
           5     0.9354    0.9872    0.9606     13243
           6     0.9732    0.8800    0.9242      1691
           7     0.9326    0.9381    0.9354      5885
           8     0.9680    0.8813    0.9226      1955

    accuracy                         0.9853    269372
   macro avg     0.9603    0.9411    0.9500    269372
weighted avg     0.9854    0.9853    0.9852    269372

Evaluation Report on Validation Data:
              precision    recall  f1-score   support

           0     0.9617    0.9847    0.9731     50662
           1     0.6950    0.6996    0.6973      3169
           2     0.7670    0.7118    0.7383      2779
           3     0.7738    0.6846    0.7265      2679
           4     0.8176    0.4995    0.6202      1059
           5     0.7907    0.8275    0.8087      3588
           6     0.8821    0.5849    0.7034       371
           7     0.7389    0.6692    0.7023      1463
           8     0.8338    0.5240    0.6435       584

    accuracy                         0.9180     66354
   macro avg     0.8067    0.6873    0.7348     66354
weighted avg     0.9152    0.9180    0.9152     66354

Evaluation Report on Test Data:
              precision    recall  f1-score   support

           0     0.9548    0.9719    0.9633     47232
           1     0.6404    0.6250    0.6326      2875
           2     0.7315    0.6770    0.7032      2511
           3     0.7279    0.6774    0.7017      3487
           4     0.6753    0.5460    0.6038      1284
           5     0.7038    0.7900    0.7444      2938
           6     0.7063    0.2736    0.3944       413
           7     0.5977    0.4924    0.5400      1255
           8     0.4418    0.4625    0.4519       320

    accuracy                         0.8933     62315
   macro avg     0.6866    0.6129    0.6373     62315
weighted avg     0.8895    0.8933    0.8902     62315

