DatasetDict({
    train: Dataset({
        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],
        num_rows: 14041
    })
    validation: Dataset({
        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],
        num_rows: 3250
    })
    test: Dataset({
        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],
        num_rows: 3453
    })
})
{'input_ids': [101, 7270, 22961, 1528, 1840, 1106, 21423, 1418, 2495, 12913, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}
Class Weights: tensor([0.0020, 0.0511, 0.0744, 0.0533, 0.0910, 0.0472, 0.2913, 0.0980, 0.2918])
Training in epoch_size: 5, batch_size: 4, learning_rate: 3e-05, device: cuda
vocab_size=28996, embed_size=768, num_layers=24, heads=8, forward_expansion=4, dropout=0.1, max_len=512, num_classes=9
Epoch 1, Loss: 0.7983
Epoch 2, Loss: 0.5641
Epoch 3, Loss: 0.4747
Epoch 4, Loss: 0.4165
Epoch 5, Loss: 0.3745
Evaluation Report:
              precision    recall  f1-score   support

           0     0.9337    0.9762    0.9545     47232
           1     0.3208    0.2379    0.2732      2875
           2     0.4031    0.5006    0.4466      2511
           3     0.3820    0.5931    0.4647      3487
           4     0.0000    0.0000    0.0000      1284
           5     0.7749    0.3598    0.4914      2938
           6     0.0000    0.0000    0.0000       413
           7     0.4718    0.3203    0.3816      1255
           8     0.2708    0.0406    0.0707       320

    accuracy                         0.8279     62315
   macro avg     0.3952    0.3365    0.3425     62315
weighted avg     0.8075    0.8279    0.8113     62315

